{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-class Reading: Day 17 (Nov 06, 2023)<br>Monte Carlo Methods 2\n",
    "Learning goals\n",
    "1. Apply Monte Carlo error propagation methods to determine the error on a calculated quantity\n",
    "1. Apply Markov Chain Monte Carlo methods to simple systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *No self-assessment questions for this reading*\n",
    "\n",
    "Similar to the previous reading assignment, the ideas presented here will mostly be new so please engage with the entire reading assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.1 Intro to Monte Carlo Error Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo error propagation starts with the notion that one representation of a measurement uncertainty (aka measurement error) is that of a gaussian where the mean of the gaussian represents the measurand (the value) and the standard deviation represents the measurement error. \n",
    "\n",
    "For example, to determine the resistance ($R$) of a certain portion of my very noisy circuit, I can measure the voltage drop ($V$) across that resistance and the current ($I$) through that resistance. In my example, let's say I measured the following values:\n",
    "* $V=(1.52 \\pm 0.11)\\mbox{ V}$, and\n",
    "* $I=(0.0642 \\pm 0.0029)\\mbox{ A}$.\n",
    "\n",
    "As a reminder, the calculus method for error propagation for a function ($f$) determined from uncorrelated variables ($x_1, x_2, ...$), the error $df$ is given by:\n",
    "\n",
    "$$f = f(x_1, x_2, ..),$$\n",
    "\n",
    "$$df = \\sqrt{\\sum^N_{i=1} (dx_i\\frac{\\partial f}{\\partial x_i})^2}.$$\n",
    "\n",
    "To find the erorr in the resistance, this would look like:\n",
    "\n",
    "$$ R = \\frac{V}{I},$$\n",
    "\n",
    "$$ dR = \\sqrt{(dV\\frac{\\partial R}{\\partial V})^2 + (dI\\frac{\\partial R}{\\partial I})^2},$$\n",
    "\n",
    "$$ dR = \\sqrt{(dV \\cdot \\frac{1}{I})^2 + (dI\\frac{V}{I^2})^2},$$\n",
    "\n",
    "$$ dR = R \\sqrt{(\\frac{dV}{V})^2 + (\\frac{dI}{I})^2},$$\n",
    "\n",
    "where the final step requires a little algegra and a substitution of $R=V/I$ back in. \n",
    "\n",
    "If you work through the arithmetic using the numbers give, you should get\n",
    "\n",
    "$$R = (23.7 \\pm 2.0)\\, \\Omega.$$\n",
    "\n",
    "Again, with the notion that the error in a measurement can be represented as the standard deviation of a Gaussian, we could represent each of these values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to generate graphical representations of these measurements\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Gaussian function\n",
    "def gaussian(x, mu, sigma):\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- 0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "# Given and calculated measurements\n",
    "V, dV = 1.52, 0.11 # V\n",
    "I, dI = 0.0642, 0.0029 # A\n",
    "R, dR = 23.7, 2.0 # ohms\n",
    "\n",
    "# Generate x and Gaussian y values\n",
    "x_V = np.linspace(0, V + 4*dV, 1000)\n",
    "x_I = np.linspace(0, I + 4*dI, 1000)\n",
    "x_R = np.linspace(0, R + 4*dR, 1000)\n",
    "y_V = gaussian(x_V, V, dV)\n",
    "y_I = gaussian(x_I, I, dI)\n",
    "y_R = gaussian(x_R, R, dR)\n",
    "\n",
    "# Make our plots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 9))\n",
    "\n",
    "axes[0].plot(x_V, y_V)\n",
    "axes[0].fill_between(x_V, y_V, alpha=0.4)\n",
    "axes[0].set_title(f\"$V = ({V} \\pm {dV})$ V\")\n",
    "axes[0].set_xlabel(\"Voltage (V)\")\n",
    "\n",
    "axes[1].plot(x_I, y_I)\n",
    "axes[1].fill_between(x_I, y_I, alpha=0.4)\n",
    "axes[1].set_title(f\"$I = ({I} \\pm {dI})$ A\")\n",
    "axes[1].set_xlabel(\"Current (A)\")\n",
    "\n",
    "axes[2].plot(x_R, y_R)\n",
    "axes[2].fill_between(x_R, y_R, alpha=0.4)\n",
    "axes[2].set_title(f\"$R = ({R} \\pm {dR})\\,\\Omega$ \")\n",
    "axes[2].set_xlabel(\"Resistance ($\\Omega$)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(True)   \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, another approach to this error propagation would be to use Monte-Carlo approaches to generate a bunch of $V$ and $I$ data drawn from the above distribution and then for each pair of $V$ and $I$ points, calculate $R$. The mean and standard devation of this $R$ distribution should then be $R \\pm dR$:\n",
    "\n",
    "$$R_{\\mbox{Monte Carlo}} = \\frac{V_{\\mbox{Monte Carlo}}}{I_{\\mbox{Monte Carlo}}}.$$\n",
    "\n",
    "Note that we are using only 5000 data points here so that the graphs below show our Monte Carlo data clearly. However, it would be reasonable to make `N` as high as one million (1,000,000) to make this quite accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "\n",
    "# Draw random samples from Gaussian distributions for V and R\n",
    "V_samples = np.random.normal(V, dV, N)\n",
    "I_samples = np.random.normal(I, dI, N)\n",
    "\n",
    "# Calculate R for each pair of V and I samples\n",
    "R_samples = V_samples / I_samples \n",
    "\n",
    "# Compute mean and standard deviation of the R samples\n",
    "R_mc = np.mean(R_samples)\n",
    "dR_mc = np.std(R_samples, ddof=1)\n",
    "\n",
    "print(f\"Calculated R = {R_mc:.1f} ± {dR_mc:.1f} ohms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these like we did above to see them instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make our plots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 9))\n",
    "\n",
    "# Plot Gaussian and histogram for V\n",
    "axes[0].plot(x_V, y_V)\n",
    "axes[0].fill_between(x_V, y_V, alpha=0.4)\n",
    "axes[0].hist(V_samples, bins=50, density=True, alpha=0.8)\n",
    "axes[0].set_title(f\"$V = ({V} \\pm {dV})$ V\")\n",
    "axes[0].set_xlabel(\"Voltage (V)\")\n",
    "\n",
    "# Plot Gaussian and histogram for I\n",
    "axes[1].plot(x_I, y_I)\n",
    "axes[1].fill_between(x_I, y_I, alpha=0.4)\n",
    "axes[1].hist(I_samples, bins=50, density=True, alpha=0.8)\n",
    "axes[1].set_title(f\"$I = ({I} \\pm {dI})$ A\")\n",
    "axes[1].set_xlabel(\"Current (A)\")\n",
    "\n",
    "# Plot Gaussian and histogram for R\n",
    "axes[2].plot(x_R, y_R)\n",
    "axes[2].fill_between(x_R, y_R, alpha=0.4)\n",
    "axes[2].hist(R_samples, bins=50, density=True, alpha=0.8)\n",
    "axes[2].set_title(f\"$R(calculated) = ({R} \\pm {dR})\\,\\Omega$;    $R(Monte Carlo) = ({R_mc:.1f} \\pm {dR_mc:.1f})\\,\\Omega$ \")\n",
    "axes[2].set_xlabel(\"Resistance ($\\Omega$)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that Monte Carlo error propagation gives a consistent result $dz$ to that from the calculus method of error progagation for the following situation.\n",
    "\n",
    "Population growth $z$ is predicted by a combination of available resources $x$ and a predator-prey relationship $y$ as given by the following expression:\n",
    "\n",
    "$$ z = \\ln x + y^2.$$\n",
    "\n",
    "Using the calculus method for error derivation, we would find that the uncertainty in this population growth is given by\n",
    "\n",
    "$$ dz = \\sqrt{\\left(\\frac{dx}{x}\\right)^2 + (2\\,y\\,dy)^2}.$$\n",
    "\n",
    "For the following values, use Monte Carlo error propagation to show that you get the same $dz$ as we get from the direct calculation:\n",
    "\n",
    "* $x = 2.03 \\pm 0.15$,\n",
    "* $y = 0.679 \\pm 0.052$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find the directly calculated values\n",
    "\n",
    "x_val, dx_val = 2.03, 0.15\n",
    "y_val, dy_val = 0.679, 0.052\n",
    "\n",
    "z_calc = np.log(x_val) + y_val**2\n",
    "dz_calc = np.sqrt((dx_val/x_val)**2 + (2*dy_val*y_val)**2)\n",
    "\n",
    "print(f\"Calculated z = {z_calc:.2f} ± {dz_calc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Monte Carlo error propagation\n",
    "\n",
    "N = 1000000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.2 Introduction to the Monte Carlo Markov Chain Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain Monte Carlo methods are algorithms where the the progression of the state depends only on the previous state vector and not the entire history of the state. In the following sections we will build up to a simple model of an ideal gas with quantized energy states by first starting out with modelling the random walk problem.\n",
    "\n",
    "The general approach is as follows (it will make much more sense when we look at our examples):\n",
    "\n",
    "1. **Initialization**: Start from an initial state.\n",
    "2. **Proposal**: Propose a move to change to a new state based on the current one. The way this proposal happens is specified by a proposal distribution.\n",
    "3. **Acceptance**: Decide whether to accept the move to the new state or stay with the current one.\n",
    "4. **Iteration**: Repeat the proposal and acceptance steps many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.3 Random walk for one particle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following two sections draw on examples from Ayars textbook: https://freecomputerbooks.com/Computational-Physics-with-Python-by-Eric-Ayars.html*\n",
    "\n",
    "Brownian motion can model the motion of a particle, such as smoke, in a gas. Random collisions with the gas molecules cause the smoke to undergo a motion described as a random walk. We will look at a one-dimensional random walk, where at each iteration will be a step in either the positive or negative direction.\n",
    "\n",
    "Based on our general approach introduced earlier, this would look as follows:\n",
    "\n",
    "1. **Initialization**: Start from an initial state.\n",
    "  * Start the particle at `x = 0`<br><br>\n",
    "2. **Proposal**: Propose a move to change to a new state based on the current one. The way this proposal happens is specified by a proposal distribution.\n",
    "  * Randomly determine if the particle will move in the positive or negative direction, with equal probability.<br><br>\n",
    "3. **Acceptance**: Decide whether to accept the move to the new state or stay with the current one.\n",
    "  * For this initial model, we will accept all proposed moves. But you could imagine an example of the smoke particle and gas being in a room such that proposals for the smoke to move through the wall would be rejected.<br><br>\n",
    "4. **Iteration**: Repeat the proposal and acceptance steps many times.\n",
    "  * We will iterate 100 times.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me multiple times to see the different ways the system can evolve\n",
    "\n",
    "N = 100  # number of steps\n",
    "\n",
    "# Store locations after each step and times\n",
    "x = [0]\n",
    "t = range(N)\n",
    "\n",
    "# The possible choices\n",
    "dir_choices = [-1, 1]\n",
    "\n",
    "# random walk\n",
    "for i in range(1, N):\n",
    "    step = np.random.choice(dir_choices)\n",
    "    x_new = x[-1] + step # move one step in the pos/neg direction\n",
    "    x.append(x_new)\n",
    "    \n",
    "\n",
    "plt.plot(t, x)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Position')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.4 Random walk for a collection of particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now expand our random walk to 200 particles and look at how the overall behaviour of this system will evolve over time. As you have seen when you ran the single-particle code above multiple times, some particles will wander off far in the positive or negative directions and others will remain somewhat close to x = 0. You will no be surprised to learn that if we look at this behaviour for multiple particles at once, we will expect the positions of all particles at a specific time to follow a Gaussian distribution. Further to that, we can also expect that the standard deviation of this Gaussian distribution should evolve like the square root of the number of steps taken.\n",
    "\n",
    "Summarizing our approach for a collection of particles:\n",
    "\n",
    "1. **Initialization**: Start from an initial state.\n",
    "  * Start with 2000 particles at `x = 0`<br><br>\n",
    "2. **Proposal**: Propose a move to change to a new state based on the current one. The way this proposal happens is specified by a proposal distribution.\n",
    "  * Randomly determine if *each* particle will move in the positive or negative direction, with equal probability.<br><br>\n",
    "3. **Acceptance**: Decide whether to accept the move to the new state or stay with the current one.\n",
    "  * Accept all proposed moves (no bounding walls)<br><br>\n",
    "4. **Iteration**: Repeat the proposal and acceptance steps many times.\n",
    "  * We will iterate 200 times.<br><br>\n",
    "  \n",
    "Work through the code below carefully to make sense of each step. We use two visualizations to summarize the behaviour of the system. The first is a histogram of all 2000 particles in the system after the final step. Notice that the mean of this is approximately zero. Second is a history of now the mean and standard deviation of `x` for all particles evolved over time. Here we see that the mean remains approximately zero and the standard deviation evolves like the square root of the number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "nsteps = 200 # number of steps\n",
    "nparticles = 2000  # number of particles\n",
    "\n",
    "# Initial positions of all particles\n",
    "x = np.zeros(nparticles)\n",
    "\n",
    "# Times\n",
    "t = range(nsteps)\n",
    "\n",
    "# Arrays to store average and standard deviations of x of all particles at each time\n",
    "x_mean = np.zeros(nsteps)\n",
    "x_std = np.zeros(nsteps)\n",
    "\n",
    "# Evolve the system through each time step\n",
    "for i in range(1,nsteps):\n",
    "    \n",
    "    # Propose the next move for all particles at the same time\n",
    "    step = np.random.choice([-1,1], size=nparticles)\n",
    "    \n",
    "    # Accept all proposed moves\n",
    "    # - Each particle in x has its own random step\n",
    "    x = x + step\n",
    "    \n",
    "    # Characterize the new state of the system\n",
    "    x_mean[i] = np.mean(x)\n",
    "    x_std[i] = np.std(x, ddof=1)\n",
    "\n",
    "# Fit the evolution of x_std to a*std(t)  \n",
    "\n",
    "# First define the fitting function\n",
    "def fun (x, a):\n",
    "    return a * np.sqrt(x)\n",
    "\n",
    "# And then perform the fit\n",
    "popt, pcov = optimize.curve_fit(fun, t, x_std)    \n",
    "    \n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Histogram of final positions\n",
    "axs[0].hist(x, bins=20)\n",
    "axs[0].set_title(f'Histogram of Particle Positions After {nsteps} Steps')\n",
    "axs[0].set_xlabel('Position')\n",
    "axs[0].set_ylabel('Number of Particles')\n",
    "\n",
    "# Plot of mean and standard deviation over time\n",
    "axs[1].plot(t, x_mean, label='Mean Position', linestyle=':')\n",
    "axs[1].plot(t, x_std, label='Position STD')\n",
    "axs[1].plot(t, fun(t,*popt), label='fit: sqrt(t)', linestyle='--')\n",
    "axs[1].set_title('Mean and Standard Deviation of Particle Positions Over Time')\n",
    "axs[1].set_xlabel('Time step')\n",
    "axs[1].set_ylabel('Position')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.5 A quantum ideal gas with one atom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following examples are based on examples from \"Computational Physics\" (revised edition) by Mark Newman.*\n",
    "\n",
    "In this example, we will be discussing energies in quantum systems. If this is a completely new idea to you, the main idea that you need to make sense of is that our atom has discrete energy levels and we are going to do a random walk up and down between these energy levels. When a move is proposed to decrease the atom's energy from an excited state ($n>1$), it will always be accepted (systems tend toward states of lower energy). However, a move that proposes to increase the atom's energy will only be accepted sometimes, where the probability that it is accepted decreases the larger that this proposed change in energy would be with respect to the thermal energy of the system.\n",
    "\n",
    "We are going to model an ideal gas as a collection of non-interacting atoms inside a box of length L. Like you have encountered with the Bohr atom, the energy of such an atom goes like $n^2$, where $n$ is the principal quantum number, which can only take on values of 1, 2, 3, ...\n",
    "\n",
    "Formally, the energy of each atom, of mass $m$ is\n",
    "\n",
    "$$E(n) = \\frac{\\pi^2 \\hbar^2}{2 m L}n^2.$$\n",
    "\n",
    "We will choose a system of units such that we can say $m = L = \\hbar = 1$, such that the above equation reduces to\n",
    "\n",
    "$$E(n) = \\frac{\\pi^2}{2}n^2.$$\n",
    "\n",
    "We will first look at a one-atom \"ideal gas\", which is mostly nonsensical, but will help us understand the behaviour of a single atom once we make it a multi-atom gas. Our system has a total thermal energy of $k_B T$ (the Boltzmann constant times temperature gives a thermal energy) and our atom has an energy that can randomly increase or decrease many times. \n",
    "\n",
    "An increase in energy corresponding to $n=+1$ would have the factor $n^2$ increase by\n",
    "\n",
    "$$\\Delta(n^2) = (n+1)^2 - n^2 = 2n  + 1,$$ \n",
    "\n",
    "and thus the total energy would change by\n",
    "\n",
    "$$dE(\\Delta n = +1) = \\frac{\\pi^2}{2}(2n+1).$$\n",
    "\n",
    "Similarly, a decrease in energy corresponding to $n=-1$ would be\n",
    "\n",
    "$$dE(\\Delta n = -1) = \\frac{\\pi^2}{2}(-2n+1).$$\n",
    "\n",
    "Finally, we introduce the Metropolis probability which tells us how likely it is that a proposed move that would change the energy of the system by an amount $dE$ will be accepted. We won't concern ourselves with how this was derived for now. As discussed above, when the atom is in an excited state and the proposed change in energy would decrease the energy of the system, it is always accepted. When the proposed change in energy would increase the energy of the system, it is accepted with the probability\n",
    "\n",
    "$$p = e^{- k_B T (dE)}.$$\n",
    "\n",
    "Summarizing our approach for a one atom ideal gas:\n",
    "\n",
    "1. **Initialization**: Start from an initial state.\n",
    "  * Start with an atom in an initial quantum state of $n=1$.<br><br>\n",
    "2. **Proposal**: Propose a move to change to a new state based on the current one. The way this proposal happens is specified by a proposal distribution.\n",
    "  * Randomly determine if the atom will try to increase or decrease its energy. These moves are proposed with equal probability, even if they are not accepted with equal probability<br><br>\n",
    "3. **Acceptance**: Decide whether to accept the move to the new state or stay with the current one.\n",
    "  * Accept all moves that would <u>decrease</u> the energy of the system from an excited state $n>1$. Reject a move that would try to <u>decrease</u> the energy of the ground state ($n=1$). Accept a move that would <u>increase</u> the energy of the system according to the Metropolis probability. <br><br>\n",
    "4. **Iteration**: Repeat the proposal and acceptance steps many times.\n",
    "  * Iterate 200 times.<br><br>\n",
    "  \n",
    "Run the code below and observe how the particle jumps out of the ground state into the first excited state and sometimes even higher excited states over time, but continues to decay back to the ground state. Try playing around with the total thermal energy of the system `Temp` to see how this impacts the overall behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The one atom ideal gas\n",
    "\n",
    "Temp = 20 # k_B*T\n",
    "Nsteps = 200 # Number of steps\n",
    "\n",
    "# Store the initial quantum state\n",
    "n = 1\n",
    "\n",
    "# Initial energy of our gas molecule:\n",
    "#   E(n) = n^2 * pi^2 * hbar^2 / (2 * m * L^2), \n",
    "#   where hbar = m = L = 1\n",
    "E = n**2 * np.pi**2 / 2\n",
    "\n",
    "# Array to store the energy evolution over time\n",
    "E_t = np.zeros(Nsteps)\n",
    "\n",
    "# Our main loop\n",
    "for j in range(Nsteps):\n",
    "\n",
    "    # Choose the proposed move\n",
    "    step = np.random.choice([\"up\", \"down\"])\n",
    "    \n",
    "    if step == \"up\":\n",
    "        dn = 1\n",
    "        dE = (2*n + 1) * np.pi**2 / 2\n",
    "    else:\n",
    "        dn = -1\n",
    "        dE = (-2*n + 1) * np.pi**2 / 2\n",
    "\n",
    "    # Decide whether to accept the move\n",
    "    # - We reject the downward move (dn = -1) if the molecule is already in the \n",
    "    #   the ground state n = 1.\n",
    "    # - When dE is negative (\"down\"), -dE/T will be positive\n",
    "    #   and thus np.exp(-dE/T) will be > 1 and the move will \n",
    "    #   always be accepted\n",
    "    # - When dE is positive (\"up\"), -dE/T will negative\n",
    "    #   and there is a finite probability we will accept the move\n",
    "    #   given by np.exp(-dE/T)\n",
    "    if not (n == 1 and dn == -1): \n",
    "        if np.random.random() < np.exp(-dE/Temp):\n",
    "            n += dn\n",
    "            E += dE\n",
    "    \n",
    "    E_t[j] = E\n",
    "\n",
    "# Make the graph\n",
    "plt.plot(E_t)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.6 A quantum ideal gas with many atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend the previous example to a system with multiple particles (`Natoms`) and modify our algorithm so that in each step we propose only a single move by first picking one of the atoms at random and then proposing that it move up or down in energy. \n",
    "\n",
    "Notice that the system seems to reach some sort of equilibrium for the total energy of the ideal gas (all of the atoms) within approximately 10,000 steps. \n",
    "\n",
    "Again, try playing around with the temperature, but also the number of atoms. How does varying these parameters impact the equilibrium energy and how long it takes to get to that equilibrium?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multi-atom ideal gas\n",
    "\n",
    "Temp = 20 # k_B*T\n",
    "Nsteps = 50000 # Number of steps\n",
    "Natoms = 1000\n",
    "\n",
    "# Store the initial quantum states of all atoms\n",
    "n = np.ones(Natoms)\n",
    "\n",
    "# Initial energy of our our system\n",
    "#   E(n) = n^2 * pi^2 * hbar^2 / (2 * m * L^2), \n",
    "#   where hbar = m = L = 1\n",
    "E = Natoms * np.pi**2 / 2\n",
    "\n",
    "# Array to store the energy evolution over time\n",
    "E_t = np.zeros(Nsteps)\n",
    "\n",
    "# Our main loop\n",
    "for j in range(Nsteps):\n",
    "\n",
    "    # Choose which atom will have the proposed move\n",
    "    atom = np.random.randint(Natoms)\n",
    "    \n",
    "    # Choose the proposed move\n",
    "    step = np.random.choice([\"up\", \"down\"])\n",
    "    \n",
    "    if step == \"up\":\n",
    "        dn = 1\n",
    "        dE = (2*n[atom] + 1) * np.pi**2 / 2\n",
    "    else:\n",
    "        dn = -1\n",
    "        dE = (-2*n[atom] + 1) * np.pi**2 / 2\n",
    "\n",
    "    # Decide whether to accept the move\n",
    "    # - We reject the downward move (dn = -1) if the molecule is already in the \n",
    "    #   the ground state n = 1.\n",
    "    # - When dE is negative (\"down\"), -dE/T will be positive\n",
    "    #   and thus np.exp(-dE/T) will be > 1 and the move will \n",
    "    #   always be accepted\n",
    "    # - When dE is positive (\"up\"), -dE/T will negative\n",
    "    #   and there is a finite probability we will accept the move\n",
    "    #   given by np.exp(-dE/T)\n",
    "    if not (n[atom] == 1 and dn == -1): \n",
    "        if np.random.random() < np.exp(-dE/Temp):\n",
    "            n[atom] += dn\n",
    "            E += dE\n",
    "    \n",
    "    E_t[j] = E\n",
    "\n",
    "# Make the graph\n",
    "plt.plot(E_t)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Total energy of the ideal gas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Submitting this reading assignment*\n",
    "Before submitting your work, please ensure you have worked carefully through all the cells. Afterward choose: File >> Save_and_Export_Notebook_As >> HTML. This will download an HTML version of your notebook to your computer which you can upload to Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
